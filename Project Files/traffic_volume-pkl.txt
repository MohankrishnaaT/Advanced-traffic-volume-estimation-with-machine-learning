# Traffic Volume Model Training - Generate traffic_volume.pkl

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.svm import SVR
from sklearn.metrics import r2_score, mean_squared_error
import pickle
import warnings
warnings.filterwarnings('ignore')

# Load the dataset
data = pd.read_csv('weatherAus.csv')  # Update with your dataset path

print("Dataset loaded successfully!")
print(f"Dataset shape: {data.shape}")
print(f"Columns: {list(data.columns)}")

# Display basic info
print("\nFirst 5 rows:")
print(data.head())

print("\nDataset info:")
print(data.info())

# Check for missing values
print("\nMissing values before handling:")
print(data.isnull().sum())

# Handle missing values
# Fill numeric columns with mean
numeric_columns = ['temp', 'rain', 'snow']
for col in numeric_columns:
    if col in data.columns:
        data[col].fillna(data[col].mean(), inplace=True)

# Fill categorical columns with mode
categorical_columns = ['weather', 'holiday']
for col in categorical_columns:
    if col in data.columns:
        data[col].fillna(data[col].mode()[0], inplace=True)

print("\nMissing values after handling:")
print(data.isnull().sum())

# Feature Engineering - Handle date_time column if exists
if 'date_time' in data.columns:
    data['date_time'] = pd.to_datetime(data['date_time'])
    
    # Extract date and time features
    data['year'] = data['date_time'].dt.year
    data['month'] = data['date_time'].dt.month
    data['day'] = data['date_time'].dt.day
    data['hour'] = data['date_time'].dt.hour
    data['minute'] = data['date_time'].dt.minute
    data['day_of_week'] = data['date_time'].dt.dayofweek
    data['is_weekend'] = (data['date_time'].dt.dayofweek >= 5).astype(int)
    
    # Drop original date_time column
    data = data.drop('date_time', axis=1)

# Handle categorical variables with one-hot encoding
categorical_cols = data.select_dtypes(include=['object']).columns
print(f"\nCategorical columns found: {list(categorical_cols)}")

if len(categorical_cols) > 0:
    data_encoded = pd.get_dummies(data, columns=categorical_cols, drop_first=True)
else:
    data_encoded = data.copy()

print(f"Dataset shape after encoding: {data_encoded.shape}")

# Prepare features and target
# Assuming 'traffic_volume' is your target column
target_column = 'traffic_volume'

if target_column in data_encoded.columns:
    X = data_encoded.drop(target_column, axis=1)
    y = data_encoded[target_column]
else:
    print(f"Warning: '{target_column}' not found in columns.")
    print("Available columns:", list(data_encoded.columns))
    # If target column name is different, update it here
    # For example, if it's 'Traffic_Volume':
    # target_column = 'Traffic_Volume'
    # X = data_encoded.drop(target_column, axis=1)
    # y = data_encoded[target_column]

print(f"\nFeatures shape: {X.shape}")
print(f"Target shape: {y.shape}")
print(f"Feature columns: {list(X.columns)}")

# Feature Scaling
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.2, random_state=42
)

print(f"\nData split completed:")
print(f"Training set: {X_train.shape}")
print(f"Testing set: {X_test.shape}")

# Initialize different models
models = {
    'Linear Regression': LinearRegression(),
    'Decision Tree': DecisionTreeRegressor(random_state=42),
    'Random Forest': RandomForestRegressor(
        n_estimators=100, 
        random_state=42,
        max_depth=10,
        min_samples_split=5
    ),
    'KNN': KNeighborsRegressor(n_neighbors=5),
    'SVM': SVR(kernel='rbf', C=1.0, gamma='scale')
}

# Train and evaluate all models
print("\n" + "="*60)
print("TRAINING AND EVALUATING MODELS")
print("="*60)

best_model = None
best_score = -float('inf')
best_model_name = ""
model_results = {}

for name, model in models.items():
    print(f"\nTraining {name}...")
    
    # Train the model
    model.fit(X_train, y_train)
    
    # Make predictions
    y_train_pred = model.predict(X_train)
    y_test_pred = model.predict(X_test)
    
    # Calculate evaluation metrics
    train_r2 = r2_score(y_train, y_train_pred)
    test_r2 = r2_score(y_test, y_test_pred)
    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))
    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))
    
    # Store results
    model_results[name] = {
        'model': model,
        'train_r2': train_r2,
        'test_r2': test_r2,
        'train_rmse': train_rmse,
        'test_rmse': test_rmse
    }
    
    print(f"{name} Results:")
    print(f"  Train R² Score: {train_r2:.4f}")
    print(f"  Test R² Score: {test_r2:.4f}")
    print(f"  Train RMSE: {train_rmse:.4f}")
    print(f"  Test RMSE: {test_rmse:.4f}")
    
    # Update best model based on test R² score
    if test_r2 > best_score:
        best_score = test_r2
        best_model = model
        best_model_name = name

# Display best model results
print(f"\n" + "="*60)
print("BEST MODEL SELECTION")
print("="*60)
print(f"Best Model: {best_model_name}")
print(f"Best Test R² Score: {best_score:.4f}")
print(f"Best Test RMSE: {model_results[best_model_name]['test_rmse']:.4f}")

# Save the best model as traffic_volume.pkl
with open('traffic_volume.pkl', 'wb') as file:
    pickle.dump(best_model, file)

print(f"\n✅ Best model ({best_model_name}) saved as 'traffic_volume.pkl'")

# Test loading the saved model
print("\n" + "="*60)
print("TESTING SAVED MODEL")
print("="*60)

# Load and test the saved model
with open('traffic_volume.pkl', 'rb') as file:
    loaded_model = pickle.load(file)

# Make a test prediction
if len(X_test) > 0:
    sample_prediction = loaded_model.predict(X_test[:1])
    actual_value = y_test.iloc[0]
    
    print(f"✅ Model loaded successfully!")
    print(f"Sample prediction: {sample_prediction[0]:.2f}")
    print(f"Actual value: {actual_value:.2f}")
    print(f"Prediction error: {abs(sample_prediction[0] - actual_value):.2f}")

# Display model summary
print(f"\n" + "="*60)
print("MODEL SUMMARY")
print("="*60)
print(f"✅ Dataset processed: {data.shape[0]} rows, {X.shape[1]} features")
print(f"✅ Best model: {best_model_name}")
print(f"✅ Model performance: R² = {best_score:.4f}")
print(f"✅ Model saved as: traffic_volume.pkl")
print(f"✅ Ready for Flask integration!")

# Save additional info for Flask app
model_info = {
    'model_name': best_model_name,
    'feature_names': list(X.columns),
    'model_performance': {
        'r2_score': best_score,
        'rmse': model_results[best_model_name]['test_rmse']
    },
    'preprocessing_steps': [
        'Handle missing values',
        'Feature engineering for date/time',
        'One-hot encoding for categorical variables',
        'Standard scaling for numerical features'
    ]
}

with open('model_info.pkl', 'wb') as file:
    pickle.dump(model_info, file)

print(f"✅ Model information saved as: model_info.pkl")