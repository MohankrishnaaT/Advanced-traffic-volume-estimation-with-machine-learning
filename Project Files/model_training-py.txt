import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.svm import SVR
from sklearn.metrics import r2_score, mean_squared_error
import pickle
import warnings
warnings.filterwarnings('ignore')

class TrafficVolumePredictor:
    def __init__(self):
        self.model = None
        self.scaler = StandardScaler()
        self.label_encoder = LabelEncoder()
        self.feature_columns = []
        
    def load_data(self, file_path):
        """Load the dataset"""
        try:
            self.data = pd.read_csv(file_path)
            print(f"Data loaded successfully! Shape: {self.data.shape}")
            return self.data
        except FileNotFoundError:
            print("Dataset file not found. Creating sample data...")
            return self.create_sample_data()
    
    def create_sample_data(self):
        """Create sample data for demonstration"""
        np.random.seed(42)
        n_samples = 1000
        
        # Generate sample data
        data = {
            'holiday': np.random.choice([0, 1], n_samples, p=[0.8, 0.2]),
            'temp': np.random.normal(15, 10, n_samples),
            'rain': np.random.exponential(2, n_samples),
            'snow': np.random.exponential(1, n_samples) * np.random.choice([0, 1], n_samples, p=[0.8, 0.2]),
            'weather': np.random.choice(['Clear', 'Clouds', 'Rain', 'Snow', 'Mist'], n_samples),
            'year': np.random.choice([2020, 2021, 2022, 2023, 2024], n_samples),
            'month': np.random.randint(1, 13, n_samples),
            'day': np.random.randint(1, 29, n_samples),
            'hour': np.random.randint(0, 24, n_samples)
        }
        
        # Create traffic volume based on realistic patterns
        traffic_volume = []
        for i in range(n_samples):
            base_volume = 2000
            
            # Hour effect (rush hours)
            if data['hour'][i] in [7, 8, 9, 17, 18, 19]:
                base_volume *= 1.8
            elif data['hour'][i] in [10, 11, 12, 13, 14, 15, 16]:
                base_volume *= 1.3
            elif data['hour'][i] in [20, 21, 22]:
                base_volume *= 1.1
            else:
                base_volume *= 0.6
            
            # Weather effect
            if data['weather'][i] == 'Rain':
                base_volume *= 0.8
            elif data['weather'][i] == 'Snow':
                base_volume *= 0.6
            elif data['weather'][i] == 'Clear':
                base_volume *= 1.1
            
            # Holiday effect
            if data['holiday'][i] == 1:
                base_volume *= 0.7
            
            # Temperature effect
            if data['temp'][i] < 0:
                base_volume *= 0.9
            elif data['temp'][i] > 30:
                base_volume *= 0.95
            
            # Add some randomness
            base_volume += np.random.normal(0, 200)
            base_volume = max(0, base_volume)  # Ensure non-negative
            
            traffic_volume.append(base_volume)
        
        data['traffic_volume'] = traffic_volume
        self.data = pd.DataFrame(data)
        
        print("Sample data created successfully!")
        return self.data
    
    def explore_data(self):
        """Explore and visualize the data"""
        print("\n=== DATA EXPLORATION ===")
        print(f"Dataset shape: {self.data.shape}")
        print("\nFirst 5 rows:")
        print(self.data.head())
        
        print("\nDataset info:")
        print(self.data.info())
        
        print("\nStatistical summary:")
        print(self.data.describe())
        
        print("\nMissing values:")
        print(self.data.isnull().sum())
        
        # Visualizations
        plt.figure(figsize=(15, 10))
        
        # Correlation heatmap
        plt.subplot(2, 3, 1)
        numeric_data = self.data.select_dtypes(include=[np.number])
        correlation_matrix = numeric_data.corr()
        sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)
        plt.title('Correlation Heatmap')
        
        # Traffic volume distribution
        plt.subplot(2, 3, 2)
        plt.hist(self.data['traffic_volume'], bins=50, alpha=0.7, color='skyblue')
        plt.title('Traffic Volume Distribution')
        plt.xlabel('Traffic Volume')
        plt.ylabel('Frequency')
        
        # Traffic volume by hour
        plt.subplot(2, 3, 3)
        hourly_traffic = self.data.groupby('hour')['traffic_volume'].mean()
        plt.plot(hourly_traffic.index, hourly_traffic.values, marker='o')
        plt.title('Average Traffic Volume by Hour')
        plt.xlabel('Hour of Day')
        plt.ylabel('Average Traffic Volume')
        
        # Weather vs Traffic
        plt.subplot(2, 3, 4)
        weather_traffic = self.data.groupby('weather')['traffic_volume'].mean()
        plt.bar(weather_traffic.index, weather_traffic.values, color='lightgreen')
        plt.title('Average Traffic Volume by Weather')
        plt.xlabel('Weather Condition')
        plt.ylabel('Average Traffic Volume')
        plt.xticks(rotation=45)
        
        # Temperature vs Traffic
        plt.subplot(2, 3, 5)
        plt.scatter(self.data['temp'], self.data['traffic_volume'], alpha=0.5)
        plt.title('Temperature vs Traffic Volume')
        plt.xlabel('Temperature (°C)')
        plt.ylabel('Traffic Volume')
        
        # Holiday vs Traffic
        plt.subplot(2, 3, 6)
        holiday_traffic = self.data.groupby('holiday')['traffic_volume'].mean()
        plt.bar(['Working Day', 'Holiday'], holiday_traffic.values, color='orange')
        plt.title('Average Traffic Volume by Day Type')
        plt.ylabel('Average Traffic Volume')
        
        plt.tight_layout()
        plt.savefig('data_exploration.png', dpi=300, bbox_inches='tight')
        plt.show()
    
    def preprocess_data(self):
        """Preprocess the data"""
        print("\n=== DATA PREPROCESSING ===")
        
        # Handle missing values
        if self.data.isnull().sum().sum() > 0:
            print("Handling missing values...")
            # Fill numeric columns with mean
            numeric_columns = self.data.select_dtypes(include=[np.number]).columns
            for col in numeric_columns:
                if self.data[col].isnull().sum() > 0:
                    self.data[col].fillna(self.data[col].mean(), inplace=True)
            
            # Fill categorical columns with mode
            categorical_columns = self.data.select_dtypes(include=['object']).columns
            for col in categorical_columns:
                if self.data[col].isnull().sum() > 0:
                    self.data[col].fillna(self.data[col].mode()[0], inplace=True)
        
        # Encode categorical variables
        if 'weather' in self.data.columns:
            weather_mapping = {
                'Clear': 0, 'Clouds': 1, 'Rain': 2, 'Snow': 3, 
                'Mist': 4, 'Fog': 5, 'Drizzle': 6, 'Thunderstorm': 7
            }
            self.data['weather_encoded'] = self.data['weather'].map(weather_mapping)
            self.data['weather_encoded'].fillna(1, inplace=True)  # Default to 'Clouds'
        
        # Prepare features and target
        feature_columns = ['holiday', 'temp', 'rain', 'snow', 'weather_encoded', 
                          'year', 'month', 'day', 'hour']
        
        # Keep only existing columns
        self.feature_columns = [col for col in feature_columns if col in self.data.columns]
        
        self.X = self.data[self.feature_columns]
        self.y = self.data['traffic_volume']
        
        print(f"Features selected: {self.feature_columns}")
        print(f"Feature matrix shape: {self.X.shape}")
        print(f"Target vector shape: {self.y.shape}")
    
    def split_data(self, test_size=0.2, random_state=42):
        """Split the data into training and testing sets"""
        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(
            self.X, self.y, test_size=test_size, random_state=random_state
        )
        
        print(f"\nData split completed:")
        print(f"Training set: {self.X_train.shape[0]} samples")
        print(f"Testing set: {self.X_test.shape[0]} samples")
    
    def scale_features(self):
        """Scale the features"""
        self.X_train_scaled = self.scaler.fit_transform(self.X_train)
        self.X_test_scaled = self.scaler.transform(self.X_test)
        
        print("Feature scaling completed.")
    
    def train_models(self):
        """Train multiple models and compare performance"""
        print("\n=== MODEL TRAINING ===")
        
        models = {
            'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),
            'Linear Regression': LinearRegression(),
            'Decision Tree': DecisionTreeRegressor(random_state=42),
            'KNN': KNeighborsRegressor(n_neighbors=5),
            'SVR': SVR(kernel='rbf', C=100, gamma=0.1)
        }
        
        self.model_results = {}
        
        for name, model in models.items():
            print(f"\nTraining {name}...")
            
            # Train model
            if name in ['Linear Regression', 'KNN', 'SVR']:
                model.fit(self.X_train_scaled, self.y_train)
                y_pred = model.predict(self.X_test_scaled)
            else:
                model.fit(self.X_train, self.y_train)
                y_pred = model.predict(self.X_test)
            
            # Calculate metrics
            r2 = r2_score(self.y_test, y_pred)
            rmse = np.sqrt(mean_squared_error(self.y_test, y_pred))
            
            self.model_results[name] = {
                'model': model,
                'r2_score': r2,
                'rmse': rmse,
                'predictions': y_pred
            }
            
            print(f"{name} - R² Score: {r2:.4f}, RMSE: {rmse:.2f}")
        
        # Select best model
        best_model_name = max(self.model_results.keys(), 
                            key=lambda x: self.model_results[x]['r2_score'])
        self.best_model = self.model_results[best_model_name]['model']
        self.model = self.best_model
        
        print(f"\nBest model: {best_model_name}")
        print(f"Best R² Score: {self.model_results[best_model_name]['r2_score']:.4f}")
    
    def evaluate_model(self):
        """Evaluate the best model"""
        print("\n=== MODEL EVALUATION ===")
        
        # Model comparison plot
        plt.figure(figsize=(12, 8))
        
        # R² Score comparison
        plt.subplot(2, 2, 1)
        models = list(self.model_results.keys())
        r2_scores = [self.model_results[model]['r2_score'] for model in models]
        plt.bar(models, r2_scores, color='skyblue')
        plt.title('R² Score Comparison')
        plt.ylabel('R² Score')
        plt.xticks(rotation=45)
        
        # RMSE comparison
        plt.subplot(2, 2, 2)
        rmse_scores = [self.model_results[model]['rmse'] for model in models]
        plt.bar(models, rmse_scores, color='lightcoral')
        plt.title('RMSE Comparison')
        plt.ylabel('RMSE')
        plt.xticks(rotation=45)
        
        # Actual vs Predicted for best model
        plt.subplot(2, 2, 3)
        best_model_name = max(self.model_results.keys(), 
                            key=lambda x: self.model_results[x]['r2_score'])
        y_pred_best = self.model_results[best_model_name]['predictions']
        plt.scatter(self.y_test, y_pred_best, alpha=0.5)
        plt.plot([self.y_test.min(), self.y_test.max()], 
                [self.y_test.min(), self.y_test.max()], 'r--', lw=2)
        plt.xlabel('Actual Traffic Volume')
        plt.ylabel('Predicted Traffic Volume')
        plt.title(f'Actual vs Predicted ({best_model_name})')
        
        # Residuals plot
        plt.subplot(2, 2, 4)
        residuals = self.y_test - y_pred_best
        plt.scatter(y_pred_best, residuals, alpha=0.5)
        plt.axhline(y=0, color='r', linestyle='--')
        plt.xlabel('Predicted Traffic Volume')
        plt.ylabel('Residuals')
        plt.title('Residual Plot')
        
        plt.tight_layout()
        plt.savefig('model_evaluation.png', dpi=300, bbox_inches='tight')
        plt.show()
    
    def save_model(self, model_path='traffic_volume.pkl', scaler_path='scaler.pkl'):
        """Save the trained model and scaler"""
        with open(model_path, 'wb') as f:
            pickle.dump(self.model, f)
        
        with open(scaler_path, 'wb') as f:
            pickle.dump(self.scaler, f)
        
        print(f"\nModel saved to {model_path}")
        print(f"Scaler saved to {scaler_path}")
    
    def predict_sample(self, sample_data):
        """Make a prediction on sample data"""
        if isinstance(sample_data, dict):
            sample_df = pd.DataFrame([sample_data])
        else:
            sample_df = sample_data
        
        # Ensure all feature columns are present
        for col in self.feature_columns:
            if col not in sample_df.columns:
                sample_df[col] = 0
        
        sample_features = sample_df[self.feature_columns]
        
        # Scale if needed
        if hasattr(self, 'X_train_scaled'):
            sample_scaled = self.scaler.transform(sample_features)
            prediction = self.model.predict(sample_scaled)
        else:
            prediction = self.model.predict(sample_features)
        
        return prediction[0]

def main():
    """Main function to run the complete pipeline"""
    print("=== TRAFFIC VOLUME PREDICTION MODEL TRAINING ===")
    
    # Initialize predictor
    predictor = TrafficVolumePredictor()
    
    # Load data (or create sample data)
    data = predictor.load_data('weatherAus.csv')  # Replace with your actual data file
    
    # Explore data
    predictor.explore_data()
    
    # Preprocess data
    predictor.preprocess_data()
    
    # Split data
    predictor.split_data()
    
    # Scale features
    predictor.scale_features()
    
    # Train models
    predictor.train_models()
    
    # Evaluate model
    predictor.evaluate_model()
    
    # Save model
    predictor.save_model()
    
    # Test prediction
    sample_input = {
        'holiday': 0,
        'temp': 20.0,
        'rain': 0.0,
        'snow': 0.0,
        'weather_encoded': 1,  # Clouds
        'year': 2024,
        'month': 6,
        'day': 15,
        'hour': 8
    }
    
    prediction = predictor.predict_sample(sample_input)
    print(f"\nSample Prediction:")
    print(f"Input: {sample_input}")
    print(f"Predicted Traffic Volume: {prediction:.2f}")
    
    print("\n=== TRAINING COMPLETED ===")

if __name__ == "__main__":
    main()