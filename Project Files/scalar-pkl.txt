# Feature Scaler Training - Generate scaler.pkl

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
import pickle
import warnings
warnings.filterwarnings('ignore')

# Load the dataset
data = pd.read_csv('weatherAus.csv')  # Update with your dataset path

print("Dataset loaded for scaler training!")
print(f"Dataset shape: {data.shape}")
print(f"Columns: {list(data.columns)}")

# Display basic info
print("\nFirst 5 rows:")
print(data.head())

# Check for missing values
print("\nMissing values before handling:")
print(data.isnull().sum())

# Handle missing values (same as in model training)
# Fill numeric columns with mean
numeric_columns = ['temp', 'rain', 'snow']
for col in numeric_columns:
    if col in data.columns:
        data[col].fillna(data[col].mean(), inplace=True)

# Fill categorical columns with mode
categorical_columns = ['weather', 'holiday']
for col in categorical_columns:
    if col in data.columns:
        data[col].fillna(data[col].mode()[0], inplace=True)

print("\nMissing values after handling:")
print(data.isnull().sum())

# Feature Engineering - Handle date_time column (same as model training)
if 'date_time' in data.columns:
    data['date_time'] = pd.to_datetime(data['date_time'])
    
    # Extract date and time features
    data['year'] = data['date_time'].dt.year
    data['month'] = data['date_time'].dt.month
    data['day'] = data['date_time'].dt.day
    data['hour'] = data['date_time'].dt.hour
    data['minute'] = data['date_time'].dt.minute
    data['day_of_week'] = data['date_time'].dt.dayofweek
    data['is_weekend'] = (data['date_time'].dt.dayofweek >= 5).astype(int)
    
    # Drop original date_time column
    data = data.drop('date_time', axis=1)
    
    print("✅ Date/time features extracted")

# Handle categorical variables with one-hot encoding (same as model training)
categorical_cols = data.select_dtypes(include=['object']).columns
print(f"\nCategorical columns found: {list(categorical_cols)}")

if len(categorical_cols) > 0:
    data_encoded = pd.get_dummies(data, columns=categorical_cols, drop_first=True)
    print("✅ Categorical variables encoded")
else:
    data_encoded = data.copy()

print(f"Dataset shape after encoding: {data_encoded.shape}")

# Prepare features (exclude target column)
target_column = 'traffic_volume'

if target_column in data_encoded.columns:
    X = data_encoded.drop(target_column, axis=1)
    y = data_encoded[target_column]
else:
    print(f"Warning: '{target_column}' not found in columns.")
    print("Available columns:", list(data_encoded.columns))
    # If target column name is different, update it here
    # For example, if it's 'Traffic_Volume':
    # target_column = 'Traffic_Volume'
    # X = data_encoded.drop(target_column, axis=1)

print(f"\nFeatures for scaling:")
print(f"Shape: {X.shape}")
print(f"Columns: {list(X.columns)}")

# Display feature statistics before scaling
print(f"\nFeature statistics BEFORE scaling:")
print(X.describe())

# Initialize and fit the StandardScaler
print(f"\n" + "="*60)
print("TRAINING FEATURE SCALER")
print("="*60)

scaler = StandardScaler()

# Fit the scaler on all features
scaler.fit(X)

print("✅ StandardScaler fitted on training data")

# Display scaler parameters
print(f"\nScaler parameters:")
print(f"Feature means: {scaler.mean_[:5]}...")  # Show first 5
print(f"Feature scales: {scaler.scale_[:5]}...")  # Show first 5
print(f"Number of features: {scaler.n_features_in_}")

# Test the scaler
X_scaled = scaler.transform(X)
X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)

print(f"\nFeature statistics AFTER scaling:")
print(X_scaled_df.describe())

# Verify scaling worked correctly (mean should be ~0, std should be ~1)
print(f"\nScaling verification:")
print(f"Mean of scaled features: {X_scaled_df.mean().mean():.6f} (should be ~0)")
print(f"Std of scaled features: {X_scaled_df.std().mean():.6f} (should be ~1)")

# Save the scaler as scaler.pkl
with open('scaler.pkl', 'wb') as file:
    pickle.dump(scaler, file)

print(f"\n✅ Feature scaler saved as 'scaler.pkl'")

# Test loading the saved scaler
print(f"\n" + "="*60)
print("TESTING SAVED SCALER")
print("="*60)

# Load and test the saved scaler
with open('scaler.pkl', 'rb') as file:
    loaded_scaler = pickle.load(file)

print("✅ Scaler loaded successfully!")

# Test with sample data
if len(X) > 0:
    # Take first row as sample
    sample_data = X.iloc[:1]  # Keep as DataFrame
    
    print(f"\nSample original data:")
    print(sample_data)
    
    # Scale using loaded scaler
    sample_scaled = loaded_scaler.transform(sample_data)
    
    print(f"\nSample scaled data:")
    print(sample_scaled)
    
    # Verify inverse transform works
    sample_inverse = loaded_scaler.inverse_transform(sample_scaled)
    
    print(f"\nSample inverse transformed data:")
    print(sample_inverse)
    
    # Check if inverse transform recovers original values
    original_values = sample_data.values
    recovered_values = sample_inverse
    difference = np.abs(original_values - recovered_values).max()
    
    print(f"\nInverse transform verification:")
    print(f"Maximum difference: {difference:.10f} (should be very small)")
    print("✅ Inverse transform working correctly!" if difference < 1e-10 else "❌ Inverse transform issue!")

# Save scaler information for Flask app reference
scaler_info = {
    'scaler_type': 'StandardScaler',
    'n_features': scaler.n_features_in_,
    'feature_names': list(X.columns),
    'feature_means': scaler.mean_.tolist(),
    'feature_scales': scaler.scale_.tolist(),
    'usage_notes': [
        'Always apply scaler.transform() to user input before model prediction',
        'Do not use scaler.fit_transform() on new data - only transform()',
        'Ensure input data has same feature order as training data',
        'Handle missing values in input data before scaling'
    ]
}

with open('scaler_info.pkl', 'wb') as file:
    pickle.dump(scaler_info, file)

print(f"\n✅ Scaler information saved as 'scaler_info.pkl'")

# Create sample input format for Flask app
print(f"\n" + "="*60)
print("FLASK APP INTEGRATION GUIDE")
print("="*60)

print("Expected input format for Flask app:")
print(f"Number of features: {len(X.columns)}")
print("Feature names and order:")
for i, feature in enumerate(X.columns, 1):
    print(f"  {i:2d}. {feature}")

# Create a sample input dictionary
sample_input_dict = {}
for col in X.columns:
    if 'temp' in col.lower():
        sample_input_dict[col] = 25.0
    elif 'rain' in col.lower():
        sample_input_dict[col] = 0.0
    elif 'snow' in col.lower():
        sample_input_dict[col] = 0.0
    elif 'hour' in col.lower():
        sample_input_dict[col] = 12
    elif 'month' in col.lower():
        sample_input_dict[col] = 6
    elif 'day' in col.lower():
        sample_input_dict[col] = 15
    else:
        sample_input_dict[col] = 0

print(f"\nSample input dictionary for Flask:")
print(sample_input_dict)

# Save sample input for reference
with open('sample_input.pkl', 'wb') as file:
    pickle.dump(sample_input_dict, file)

print(f"\n✅ Sample input saved as 'sample_input.pkl'")

# Summary
print(f"\n" + "="*60)
print("SCALER TRAINING SUMMARY")
print("="*60)
print(f"✅ Dataset processed: {data.shape[0]} rows")
print(f"✅ Features scaled: {X.shape[1]} features")
print(f"✅ Scaler type: StandardScaler")
print(f"✅ Files generated:")
print(f"    - scaler.pkl (fitted StandardScaler)")
print(f"    - scaler_info.pkl (scaler metadata)")
print(f"    - sample_input.pkl (sample input format)")
print(f"✅ Ready for Flask integration!")

print(f"\nFlask usage example:")
print(f"```python")
print(f"import pickle")
print(f"import pandas as pd")
print(f"")
print(f"# Load scaler")
print(f"with open('scaler.pkl', 'rb') as f:")
print(f"    scaler = pickle.load(f)")
print(f"")
print(f"# Prepare user input (same feature order as training)")
print(f"user_input = pd.DataFrame([user_data_dict])")
print(f"")
print(f"# Scale the input")
print(f"scaled_input = scaler.transform(user_input)")
print(f"")
print(f"# Use scaled_input for model prediction")
print(f"```")